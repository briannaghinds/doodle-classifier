{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT implementation (node classification) - Cora Dataset\n",
    "\n",
    "https://arxiv.org/pdf/1710.10903 (GAT Research Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# torch libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.optim import Adam \n",
    "from torch.nn import Linear, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data():\n",
    "    # get the Cora dataset\n",
    "    \"\"\"CORA DATASET:\n",
    "        - the Cora datset is a large graph network that shows 2708 scientific publications\n",
    "        our goal is to classify it into 1 of 7 classes\n",
    "    \"\"\"\n",
    "    dataset = Planetoid(root=\".\", name=\"Cora\")\n",
    "    cora = dataset[0]\n",
    "\n",
    "    print(cora)\n",
    "\n",
    "    return dataset, cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "dataset, data = gather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT class\n",
    "\"\"\"METHODS:\n",
    "    __init__ : initializes my custom GAT model\n",
    "    forward : the defined forward pass method for my GAT model\n",
    "    train : training loop that prints out the model loss per 10th epoch\n",
    "    test : testing loop that returns the final model accuracy\n",
    "\"\"\"\n",
    "class CoraGAT(torch.nn.Module):\n",
    "    def __init__(self, input, hidden, output, heads=8):\n",
    "        \"\"\"PARAMETERS:\n",
    "            input : the number of features for each node in the graph\n",
    "            hidden : size of the hidden layer (needs to be tuned) -> defined as hidden*heads\n",
    "            output : the number of output features\n",
    "            heads : defined to be 8, number of attention heads (each head learns a different type of attention pattern)\n",
    "        \"\"\"\n",
    "        super(CoraGAT, self).__init__()\n",
    "\n",
    "        # first GAT layer with 8 attention heads\n",
    "        self.conv1 = GATConv(\n",
    "            input,\n",
    "            hidden,\n",
    "            heads=heads,\n",
    "            dropout=0.6\n",
    "        )\n",
    "\n",
    "        # second GAT layer with 1 attention head\n",
    "        self.conv2 = GATConv(\n",
    "            hidden*heads,\n",
    "            output,\n",
    "            heads=1,\n",
    "            concat=False,\n",
    "            dropout=0.6\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # first GAT layer\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # second GAT layer\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    # analysis methods\n",
    "    def train_model(self, data, epochs):\n",
    "        # initialize the optimizer\n",
    "        optimizer = Adam(self.parameters(), lr=0.005)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # get model prediction \n",
    "            out = self(data.x, data.edge_index)\n",
    "\n",
    "            # calculate loss amount\n",
    "            loss_fn = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "            # backpropagate through the GAT\n",
    "            loss_fn.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ###### PRINTOUTS EVERYE 10th EPOCH\n",
    "            if i%10==0:\n",
    "                print(f\"Epochs: {i}/{epochs}\\tLoss:{loss_fn.item()}\")\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_model(self, data):\n",
    "        self.eval()\n",
    "\n",
    "        out = self(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "        accuracy = int(correct.sum()) / int(data.test_mask.sum())\n",
    "\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "gat = CoraGAT(dataset.num_features, hidden=8, output=dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 0/1000\tLoss:1.9963725805282593\n",
      "Epochs: 10/1000\tLoss:1.2424739599227905\n",
      "Epochs: 20/1000\tLoss:0.8960310816764832\n",
      "Epochs: 30/1000\tLoss:0.7475935816764832\n",
      "Epochs: 40/1000\tLoss:0.6328566670417786\n",
      "Epochs: 50/1000\tLoss:0.6520478129386902\n",
      "Epochs: 60/1000\tLoss:0.5875628590583801\n",
      "Epochs: 70/1000\tLoss:0.5228483080863953\n",
      "Epochs: 80/1000\tLoss:0.45635709166526794\n",
      "Epochs: 90/1000\tLoss:0.4738793969154358\n",
      "Epochs: 100/1000\tLoss:0.33258935809135437\n",
      "Epochs: 110/1000\tLoss:0.3398807942867279\n",
      "Epochs: 120/1000\tLoss:0.453147292137146\n",
      "Epochs: 130/1000\tLoss:0.25478512048721313\n",
      "Epochs: 140/1000\tLoss:0.4987379312515259\n",
      "Epochs: 150/1000\tLoss:0.4391951560974121\n",
      "Epochs: 160/1000\tLoss:0.4094718396663666\n",
      "Epochs: 170/1000\tLoss:0.2776276469230652\n",
      "Epochs: 180/1000\tLoss:0.3151956796646118\n",
      "Epochs: 190/1000\tLoss:0.2404175102710724\n",
      "Epochs: 200/1000\tLoss:0.37491241097450256\n",
      "Epochs: 210/1000\tLoss:0.3914780020713806\n",
      "Epochs: 220/1000\tLoss:0.3592813014984131\n",
      "Epochs: 230/1000\tLoss:0.2518371045589447\n",
      "Epochs: 240/1000\tLoss:0.3054772913455963\n",
      "Epochs: 250/1000\tLoss:0.37262243032455444\n",
      "Epochs: 260/1000\tLoss:0.1623198539018631\n",
      "Epochs: 270/1000\tLoss:0.3460584878921509\n",
      "Epochs: 280/1000\tLoss:0.35565486550331116\n",
      "Epochs: 290/1000\tLoss:0.41544589400291443\n",
      "Epochs: 300/1000\tLoss:0.2954533100128174\n",
      "Epochs: 310/1000\tLoss:0.4000038802623749\n",
      "Epochs: 320/1000\tLoss:0.3469335436820984\n",
      "Epochs: 330/1000\tLoss:0.37634989619255066\n",
      "Epochs: 340/1000\tLoss:0.37118735909461975\n",
      "Epochs: 350/1000\tLoss:0.34649211168289185\n",
      "Epochs: 360/1000\tLoss:0.34276968240737915\n",
      "Epochs: 370/1000\tLoss:0.34231698513031006\n",
      "Epochs: 380/1000\tLoss:0.27000412344932556\n",
      "Epochs: 390/1000\tLoss:0.3984052836894989\n",
      "Epochs: 400/1000\tLoss:0.3135155141353607\n",
      "Epochs: 410/1000\tLoss:0.33246850967407227\n",
      "Epochs: 420/1000\tLoss:0.31234386563301086\n",
      "Epochs: 430/1000\tLoss:0.22468368709087372\n",
      "Epochs: 440/1000\tLoss:0.3103983700275421\n",
      "Epochs: 450/1000\tLoss:0.3128470182418823\n",
      "Epochs: 460/1000\tLoss:0.24663139879703522\n",
      "Epochs: 470/1000\tLoss:0.34199294447898865\n",
      "Epochs: 480/1000\tLoss:0.4129006564617157\n",
      "Epochs: 490/1000\tLoss:0.3959313929080963\n",
      "Epochs: 500/1000\tLoss:0.23714648187160492\n",
      "Epochs: 510/1000\tLoss:0.4139593541622162\n",
      "Epochs: 520/1000\tLoss:0.30888843536376953\n",
      "Epochs: 530/1000\tLoss:0.34965288639068604\n",
      "Epochs: 540/1000\tLoss:0.34159988164901733\n",
      "Epochs: 550/1000\tLoss:0.33044514060020447\n",
      "Epochs: 560/1000\tLoss:0.33765745162963867\n",
      "Epochs: 570/1000\tLoss:0.31891295313835144\n",
      "Epochs: 580/1000\tLoss:0.4369843006134033\n",
      "Epochs: 590/1000\tLoss:0.4518609941005707\n",
      "Epochs: 600/1000\tLoss:0.3137744069099426\n",
      "Epochs: 610/1000\tLoss:0.2888738811016083\n",
      "Epochs: 620/1000\tLoss:0.2731514573097229\n",
      "Epochs: 630/1000\tLoss:0.2743714451789856\n",
      "Epochs: 640/1000\tLoss:0.41868606209754944\n",
      "Epochs: 650/1000\tLoss:0.32954514026641846\n",
      "Epochs: 660/1000\tLoss:0.30806466937065125\n",
      "Epochs: 670/1000\tLoss:0.2048521786928177\n",
      "Epochs: 680/1000\tLoss:0.32741227746009827\n",
      "Epochs: 690/1000\tLoss:0.3047315776348114\n",
      "Epochs: 700/1000\tLoss:0.38448673486709595\n",
      "Epochs: 710/1000\tLoss:0.3929274380207062\n",
      "Epochs: 720/1000\tLoss:0.28617292642593384\n",
      "Epochs: 730/1000\tLoss:0.37730109691619873\n",
      "Epochs: 740/1000\tLoss:0.3523404598236084\n",
      "Epochs: 750/1000\tLoss:0.36268144845962524\n",
      "Epochs: 760/1000\tLoss:0.31290093064308167\n",
      "Epochs: 770/1000\tLoss:0.37564003467559814\n",
      "Epochs: 780/1000\tLoss:0.36086204648017883\n",
      "Epochs: 790/1000\tLoss:0.28449639678001404\n",
      "Epochs: 800/1000\tLoss:0.3247954547405243\n",
      "Epochs: 810/1000\tLoss:0.4546312391757965\n",
      "Epochs: 820/1000\tLoss:0.2702152729034424\n",
      "Epochs: 830/1000\tLoss:0.3143154978752136\n",
      "Epochs: 840/1000\tLoss:0.3579588532447815\n",
      "Epochs: 850/1000\tLoss:0.2612398564815521\n",
      "Epochs: 860/1000\tLoss:0.28645727038383484\n",
      "Epochs: 870/1000\tLoss:0.24644137918949127\n",
      "Epochs: 880/1000\tLoss:0.2958567440509796\n",
      "Epochs: 890/1000\tLoss:0.2774884104728699\n",
      "Epochs: 900/1000\tLoss:0.34228625893592834\n",
      "Epochs: 910/1000\tLoss:0.3438829779624939\n",
      "Epochs: 920/1000\tLoss:0.4596317410469055\n",
      "Epochs: 930/1000\tLoss:0.2883222997188568\n",
      "Epochs: 940/1000\tLoss:0.21472100913524628\n",
      "Epochs: 950/1000\tLoss:0.37889575958251953\n",
      "Epochs: 960/1000\tLoss:0.38808783888816833\n",
      "Epochs: 970/1000\tLoss:0.20616514980793\n",
      "Epochs: 980/1000\tLoss:0.35399118065834045\n",
      "Epochs: 990/1000\tLoss:0.3567000925540924\n"
     ]
    }
   ],
   "source": [
    "# print out training loop\n",
    "gat.train_model(data, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT Model Accuracy: 0.767\n"
     ]
    }
   ],
   "source": [
    "# print out testing loop\n",
    "acc = gat.test_model(data)\n",
    "print(f\"GAT Model Accuracy: {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
